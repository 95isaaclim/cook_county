{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from ds100_utils import *\n",
    "from feature_func import *\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('cook_county_data.zip') as item:\n",
    "    item.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_val_data = pd.read_csv(\"cook_county_train_val.csv\", index_col='Unnamed: 0')\n",
    "test_data = pd.read_csv(\"cook_county_contest_test.csv\", index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "def train_val_split(data):\n",
    "    \"\"\" \n",
    "    Takes in a DataFrame `data` and randomly splits it into two smaller DataFrames \n",
    "    named `train` and `validation` with 80% and 20% of the data, respectively. \n",
    "    \"\"\"\n",
    "    \n",
    "    data_len = data.shape[0]\n",
    "    shuffled_indices = np.random.permutation(data_len)\n",
    "\n",
    "    train_len = int(data_len * 0.8)\n",
    "    validation_len = data_len - train_len\n",
    "    \n",
    "    train_indices = shuffled_indices[:train_len]\n",
    "    validation_indices = shuffled_indices[train_len:]\n",
    "\n",
    "    train = data.iloc[train_indices]\n",
    "    validation = data.iloc[validation_indices]\n",
    "   \n",
    "    return train, validation\n",
    "train, validation = train_val_split(training_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_func import *    # Import functions from Part 1\n",
    "import re\n",
    "\n",
    "def add_total_bedrooms(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): a DataFrame containing at least the Description column.\n",
    "\n",
    "    Output:\n",
    "      a Dataframe with a new column \"Bedrooms\" containing ints.\n",
    "\n",
    "    \"\"\"\n",
    "    with_rooms = data.copy()\n",
    "    patt = r'(\\d+) of which are bedrooms'\n",
    "    with_rooms['Bedrooms'] = with_rooms['Description'].apply(\n",
    "        lambda n: int(re.search(patt, n).group(1)) if re.search(patt, n) \n",
    "        else 0\n",
    "    )\n",
    "    \n",
    "    return with_rooms\n",
    "\n",
    "def feature_engine_simple(data):\n",
    "    # Remove outliers\n",
    "    data = remove_outliers(data, 'Sale Price', lower=499)\n",
    "    # Create Log Sale Price column\n",
    "    data = log_transform(data, 'Sale Price')\n",
    "    # Create Bedroom column\n",
    "    data = add_total_bedrooms(data)\n",
    "    # Select X and Y from the full data\n",
    "    X = data[['Bedrooms']]\n",
    "    Y = data['Log Sale Price']\n",
    "    return X, Y\n",
    "\n",
    "# Reload the data\n",
    "full_data = pd.read_csv(\"cook_county_train.csv\")\n",
    "\n",
    "# Process the data using the pipeline for the first model.\n",
    "np.random.seed(1337)\n",
    "train_m1, valid_m1 = train_val_split(full_data)\n",
    "X_train_m1_simple, Y_train_m1_simple = feature_engine_simple(train_m1)\n",
    "X_valid_m1_simple, Y_valid_m1_simple = feature_engine_simple(valid_m1)\n",
    "\n",
    "display(X_train_m1_simple.head())\n",
    "display(Y_train_m1_simple.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define feature_engine_pipe\n",
    "def feature_engine_pipe(data, pipeline_functions, prediction_col):\n",
    "    \"\"\"Process the data for a guided model.\"\"\"\n",
    "    for function, arguments, keyword_arguments in pipeline_functions:\n",
    "        if keyword_arguments and (not arguments):\n",
    "            data = data.pipe(function, **keyword_arguments)\n",
    "        elif (not keyword_arguments) and (arguments):\n",
    "            data = data.pipe(function, *arguments)\n",
    "        else:\n",
    "            data = data.pipe(function)\n",
    "    X = data.drop(columns=[prediction_col])\n",
    "    Y = data.loc[:, prediction_col]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "full_data = pd.read_csv(\"cook_county_train.csv\")\n",
    "\n",
    "# Apply feature engineering to the data using the pipeline for the first model\n",
    "np.random.seed(1337)\n",
    "train_m1, valid_m1 = train_val_split(full_data)\n",
    "\n",
    "# Helper function\n",
    "def select_columns(data, *columns):\n",
    "    \"\"\"Select only columns passed as arguments.\"\"\"\n",
    "    return data.loc[:, columns]\n",
    "\n",
    "# Pipelines, a list of tuples\n",
    "m1_pipelines = [\n",
    "    (remove_outliers, None, {\n",
    "        'variable': 'Sale Price',\n",
    "        'lower': 499,\n",
    "    }),\n",
    "    (log_transform, None, {'col': 'Sale Price'}),\n",
    "    (add_total_bedrooms, None, None),\n",
    "    (select_columns, ['Log Sale Price', 'Bedrooms'], None)\n",
    "]\n",
    "\n",
    "X_train_m1, Y_train_m1 = feature_engine_pipe(train_m1, m1_pipelines, 'Log Sale Price')\n",
    "X_valid_m1, Y_valid_m1 = feature_engine_pipe(valid_m1, m1_pipelines, 'Log Sale Price')\n",
    "\n",
    "display(X_train_m1.head())\n",
    "display(Y_train_m1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "def add_log_building_sqft(data):\n",
    "    with_log_building_sqft = data.copy()\n",
    "    with_log_building_sqft['Log Building Square Feet'] = with_log_building_sqft['Building Square Feet'].apply(np.log)\n",
    "    \n",
    "    return with_log_building_sqft\n",
    "\n",
    "# Process the data using the pipeline for the second model\n",
    "train_m2, valid_m2 = train_val_split(full_data)\n",
    "\n",
    "m2_pipelines = [\n",
    "    (remove_outliers, None, {\n",
    "        'variable': 'Sale Price',\n",
    "        'lower': 499,\n",
    "    }),\n",
    "    (log_transform, None, {'col': 'Sale Price'}),\n",
    "    (add_total_bedrooms, None, None),\n",
    "    (add_log_building_sqft, None, None),\n",
    "    (select_columns, ['Log Sale Price', 'Bedrooms', 'Log Building Square Feet'], None)\n",
    "]\n",
    "\n",
    "X_train_m2, Y_train_m2 = feature_engine_pipe(train_m2, m2_pipelines, 'Log Sale Price')\n",
    "X_valid_m2, Y_valid_m2 = feature_engine_pipe(valid_m2, m2_pipelines, 'Log Sale Price')\n",
    "\n",
    "display(X_train_m2.head())\n",
    "display(Y_train_m2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_m1 = lm.LinearRegression(fit_intercept=True)\n",
    "linear_model_m2 = lm.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the 1st model\n",
    "# Compute the fitted and predicted values of Log Sale Price for 1st model\n",
    "linear_model_m1.fit(X_train_m1, Y_train_m1)\n",
    "Y_fitted_m1 = linear_model_m1.predict(X_train_m1)\n",
    "Y_predicted_m1 = linear_model_m1.predict(X_valid_m1)\n",
    "\n",
    "# Fit the 2nd model\n",
    "linear_model_m2.fit(X_train_m2, Y_train_m2)\n",
    "# Compute the fitted and predicted values of Log Sale Price for 2nd model\n",
    "Y_fitted_m2 = linear_model_m2.predict(X_train_m2)\n",
    "Y_predicted_m2 = linear_model_m2.predict(X_valid_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculates RMSE from actual and predicted values.\n",
    "    Input:\n",
    "      predicted (1D array): Vector of predicted/fitted values\n",
    "      actual (1D array): Vector of actual values\n",
    "    Output:\n",
    "      A float, the RMSE value.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_size = 0.1\n",
    "opacity = 0.1  \n",
    "\n",
    "plt.scatter(x = Y_valid_m2, y = Y_valid_m2 - Y_predicted_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f                               \n",
    "import otter                            \n",
    "grader = otter.Notebook(\"projA2.ipynb\")\n",
    "\n",
    "# Imports all the necessary libraries again\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from ds100_utils import *\n",
    "from feature_func import *\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv(\"cook_county_train.csv\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def ohe_roof_material(data):\n",
    "    \"\"\"\n",
    "    One-hot-encodes roof material. New columns are of the form \"Roof Material_MATERIAL\".\n",
    "    \"\"\"\n",
    "    cat = [\"Roof Material\"]\n",
    "    ohe = OneHotEncoder()\n",
    "    ohe.fit(data[cat])\n",
    "    cat_data = ohe.transform(data[cat]).toarray()\n",
    "    cat_df = pd.DataFrame(data = cat_data, columns = ohe.get_feature_names_out(), index = data.index)\n",
    "    return data.join(cat_df)\n",
    "\n",
    "def add_in_expensive_neighborhood(data, expensive_neighborhoods):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): a DataFrame containing a 'Neighborhood Code' column with values\n",
    "        found in the codebook\n",
    "      expensive_neighborhoods (list of ints): ints should be the neighborhood codes of\n",
    "        neighborhoods pre-identified as expensive\n",
    "    Output:\n",
    "      DataFrame identical to the input with the addition of a binary\n",
    "      in_expensive_neighborhood column\n",
    "    \"\"\"\n",
    "    data['in_expensive_neighborhood'] = data[\"Neighborhood Code\"].isin(expensive_neighborhoods).astype(\"int\")\n",
    "    return data\n",
    "\n",
    "def find_expensive_neighborhoods(data, n=3, metric=np.median):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      data (DataFrame): should contain at least an int-valued 'Neighborhood Code'\n",
    "        and a numeric 'Log Sale Price' column\n",
    "      n (int): the number of top values desired\n",
    "      metric (function): function used for aggregating the data in each neighborhood.\n",
    "        for example, np.median for median prices\n",
    "    \n",
    "    Output:\n",
    "      a list of the the neighborhood codes of the top n highest-priced neighborhoods \n",
    "      as measured by the metric function\n",
    "    \"\"\"\n",
    "    neighborhoods = data.groupby(\"Neighborhood Code\").agg({\"Log Sale Price\": metric}).sort_values(\"Log Sale Price\", ascending = False)[:n]\n",
    "    neighborhoods = neighborhoods.index    \n",
    "    # This makes sure the final list contains the generic int type used in Python3, not specific ones used in NumPy.\n",
    "    return [int(code) for code in neighborhoods]\n",
    "\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "def train_val_split(data):\n",
    "    \"\"\" \n",
    "    Takes in a DataFrame `data` and randomly splits it into two smaller DataFrames \n",
    "    named `train` and `validation` with 80% and 20% of the data, respectively. \n",
    "    \"\"\"\n",
    "    \n",
    "    data_len = data.shape[0]\n",
    "    shuffled_indices = np.random.permutation(data_len)\n",
    "\n",
    "    train_len = int(data_len * 0.8)\n",
    "    validation_len = data_len - train_len\n",
    "    \n",
    "    train_indices = shuffled_indices[:train_len]\n",
    "    validation_indices = shuffled_indices[train_len:]\n",
    "\n",
    "    train = data.iloc[train_indices]\n",
    "    validation = data.iloc[validation_indices]\n",
    "   \n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please include all of your feature engineering processes inside this function.\n",
    "# Do not modify the parameters of this function.\n",
    "def feature_engine_final(data, is_test_set=False):\n",
    "    # Whenever you access 'Log Sale Price' or 'Sale Price', make sure to use the\n",
    "    # condition is_test_set like this:\n",
    "    if not is_test_set:\n",
    "        # Processing for the training set (i.e. not the test set)\n",
    "        # CAN involve references to sale price!\n",
    "        # CAN involve filtering certain rows or removing outliers\n",
    "        data = remove_outliers(data, \"Sale Price\", lower = 3000, upper = 2000000)\n",
    "\n",
    "        data[\"Log Sale Price\"] = np.log(data[\"Sale Price\"])\n",
    "        data = add_total_bedrooms(data)\n",
    "        data[\"Log Estimate (Building)\"] = np.log(data['Estimate (Building)'] + 1)\n",
    "        data[\"Log Estimate (Land)\"] = np.log(data['Estimate (Land)'] + 1)\n",
    "        data[\"Log Building Square Feet\"] = np.log(data['Building Square Feet'] + 1)\n",
    "        data[\"Log Land Square Feet\"] = np.log(data['Land Square Feet'] + 1)\n",
    "        data[\"Log Census Tract\"] = np.log(data[\"Census Tract\"] + 1)\n",
    "        \n",
    "        data = data.loc[:, [\"Bedrooms\", \"Log Sale Price\", \"Log Estimate (Building)\", \"Log Estimate (Land)\", \"Log Building Square Feet\", \"Log Land Square Feet\", \"Log Census Tract\"]]\n",
    "        \n",
    "    else:\n",
    "        # Processing for the test set\n",
    "        # CANNOT involve references to sale price!\n",
    "        # CANNOT involve removing any rows\n",
    "        data = add_total_bedrooms(data)\n",
    "        data[\"Log Estimate (Building)\"] = np.log(data['Estimate (Building)'] + 1)\n",
    "        data[\"Log Estimate (Land)\"] = np.log(data['Estimate (Land)'] + 1)\n",
    "        data[\"Log Building Square Feet\"] = np.log(data['Building Square Feet'] + 1)\n",
    "        data[\"Log Land Square Feet\"] = np.log(data['Land Square Feet'] + 1)\n",
    "        data[\"Log Census Tract\"] = np.log(data[\"Census Tract\"] + 1)\n",
    "\n",
    "        data = data.loc[:, [\"Bedrooms\", \"Log Estimate (Building)\", \"Log Estimate (Land)\", \"Log Building Square Feet\", \"Log Land Square Feet\", \"Log Census Tract\"]]\n",
    "    \n",
    "    \n",
    "    # Return predictors (X) and response (Y) variables separately\n",
    "    if is_test_set:\n",
    "        # Predictors \n",
    "        X = data\n",
    "        return X\n",
    "    else:\n",
    "        # Predictors. Your X should not include Log Sale Price!\n",
    "        X = data.drop([\"Log Sale Price\"], axis = 1)\n",
    "        # Response variable\n",
    "        Y = data.loc[:, \"Log Sale Price\"]\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "check_rmse_threshold = run_linear_regression_test_optim(lm.LinearRegression(fit_intercept=True), feature_engine_final, 'cook_county_train.csv', None, False)\n",
    "print(\"Current training RMSE:\", check_rmse_threshold.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculates RMSE from actual and predicted values.\n",
    "    Input:\n",
    "      predicted (1D array): Vector of predicted/fitted values\n",
    "      actual (1D array): Vector of actual values\n",
    "    Output:\n",
    "      A float, the RMSE value.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((actual - predicted)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = run_linear_regression_test(lm.LinearRegression(fit_intercept=True), feature_engine_final, None, 'cook_county_train.csv', 'cook_county_contest_test.csv', \n",
    "                                         is_test = True, is_ranking = False, return_predictions = True\n",
    "                                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('cook_county_train.csv')\n",
    "X, Y_true = feature_engine_final(train_df)\n",
    "model = lm.LinearRegression(fit_intercept=True)\n",
    "model.fit(X, Y_true)\n",
    "Y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame({'True Log Sale Price' : Y_true, 'Predicted Log Sale Price' : Y_pred, \n",
    "                         'True Sale Price' : np.e**Y_true, 'Predicted Sale Price' : np.e**Y_pred})\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_Y_true, max_Y_true = np.round(np.min(Y_true), 1) , np.round(np.max(Y_true), 1)\n",
    "median_Y_true = np.round(np.median(Y_true), 1)\n",
    "cheap_df = preds_df[(preds_df['True Log Sale Price'] >= min_Y_true) & (preds_df['True Log Sale Price'] <= median_Y_true)]\n",
    "expensive_df = preds_df[(preds_df['True Log Sale Price'] > median_Y_true) & (preds_df['True Log Sale Price'] <= max_Y_true)]\n",
    "\n",
    "print(f'\\nThe lower interval contains houses with true sale price ${np.round(np.e**min_Y_true)} to ${np.round(np.e**median_Y_true)}')\n",
    "print(f'The higher interval contains houses with true sale price ${np.round(np.e**median_Y_true)} to ${np.round(np.e**max_Y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cheap = rmse(cheap_df[\"Predicted Sale Price\"], cheap_df[\"True Sale Price\"])\n",
    "rmse_expensive = rmse(expensive_df[\"Predicted Sale Price\"], expensive_df[\"True Sale Price\"])\n",
    "\n",
    "prop_overest_cheap = np.count_nonzero(cheap_df[\"Predicted Log Sale Price\"] > cheap_df[\"True Log Sale Price\"])/cheap_df.shape[0]\n",
    "prop_overest_expensive = np.count_nonzero(expensive_df[\"Predicted Log Sale Price\"] > expensive_df[\"True Log Sale Price\"])/expensive_df.shape[0]\n",
    "\n",
    "print(f\"The RMSE for properties with log sale prices in the interval {(min_Y_true, median_Y_true)} is {np.round(rmse_cheap)}\")\n",
    "print(f\"The RMSE for properties with log sale prices in the interval {(median_Y_true, max_Y_true)} is {np.round(rmse_expensive)}\\n\")\n",
    "print(f\"The percentage of overestimated values for properties with log sale prices in the interval {(min_Y_true, median_Y_true)} is {np.round(100 * prop_overest_cheap, 2)}%\")\n",
    "print(f\"The percentage of overestimated values for properties with log sale prices in the interval {(median_Y_true, max_Y_true)} is {np.round(100 * prop_overest_expensive, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_interval(df, start, end):\n",
    "    '''\n",
    "    Given a design matrix X and response vector Y, computes the RMSE for a subset of values \n",
    "    wherein the corresponding Log Sale Price lies in the interval [start, end].\n",
    "\n",
    "    Input: \n",
    "    df : pandas DataFrame with columns 'True Log Sale Price', \n",
    "        'Predicted Log Sale Price', 'True Sale Price', 'Predicted Sale Price'\n",
    "    start : A float specifying the start of the interval (inclusive)\n",
    "    end : A float specifying the end of the interval (inclusive)\n",
    "    '''\n",
    "\n",
    "    subset_df = df[(df['True Log Sale Price'] >= start) & (df['True Log Sale Price'] <= end)]\n",
    "\n",
    "    rmse_subset = rmse(subset_df[\"Predicted Sale Price\"], subset_df[\"True Sale Price\"])\n",
    "    return rmse_subset\n",
    "    \n",
    "def prop_overest_interval(df, start, end):\n",
    "    '''\n",
    "    Given a DataFrame df, computes prop_overest for a subset of values \n",
    "    wherein the corresponding Log Sale Price lies in the interval [start, end].\n",
    "\n",
    "    Input: \n",
    "    df : pandas DataFrame with columns 'True Log Sale Price', \n",
    "        'Predicted Log Sale Price', 'True Sale Price', 'Predicted Sale Price'\n",
    "    start : A float specifying the start of the interval (inclusive)\n",
    "    end : A float specifying the end of the interval (inclusive)\n",
    "    '''\n",
    "    \n",
    "    subset_df = df[(df['True Log Sale Price'] >= start) & (df['True Log Sale Price'] <= end)]\n",
    "\n",
    "    if subset_df.shape[0] == 0:\n",
    "        return -1\n",
    "\n",
    "    prop_subset = np.count_nonzero(subset_df[\"Predicted Log Sale Price\"] > subset_df[\"True Log Sale Price\"])/subset_df.shape[0]\n",
    "    return prop_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE plot\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.subplot(1, 2, 1) \n",
    "rmses = []\n",
    "for i in np.arange(8, 14, 0.5):\n",
    "    rmses.append(rmse_interval(preds_df, i, i + 0.5))\n",
    "plt.bar(x = np.arange(8.25, 14.25, 0.5), height = rmses, edgecolor = 'black', width = 0.5)\n",
    "plt.title('RMSE Over Different Intervals\\n of Log Sale Price', fontsize = 10)\n",
    "plt.xlabel('Log Sale Price')\n",
    "plt.yticks(fontsize = 10)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "# Overestimation plot  \n",
    "plt.subplot(1, 2, 2)\n",
    "props = []\n",
    "for i in np.arange(8, 14, 0.5):\n",
    "    props.append(prop_overest_interval(preds_df, i, i + 0.5) * 100) \n",
    "plt.bar(x = np.arange(8.25, 14.25, 0.5), height = props, edgecolor = 'black', width = 0.5)\n",
    "plt.title('Percentage of House Values Overestimated \\nover different intervals of Log Sale Price', fontsize = 10)\n",
    "plt.xlabel('Log Sale Price')\n",
    "plt.yticks(fontsize = 10)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.ylabel('Percentage of House Values\\n that were Overestimated (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
